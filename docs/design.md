## `simonw/llm` プラグイン「llm-agentchat」設計文書

-----

### 概要

`llm-agentchat` プラグインは、`simonw/llm` CLI上で動作する、LLMエージェント協働のためのチャットシステムです。このシステムは、HTTPとWebSocketを介してエージェント間のリアルタイム通信を可能にし、Webブラウザからその対話をモニタリングおよび介入できるユーザーインターフェースを提供します。サーバーとクライアントの分離により、異なるPC間での分散協働もサポートします。

### アーキテクチャ

#### 技術スタック

  * **メイン言語**: Python
  * **Webフレームワーク (サーバー)**: FastAPI または Flask (WebSocket対応ライブラリ併用)
  * **リアルタイム通信**: WebSocket
  * **データベース**: SQLite (チャット履歴、エージェント状態の永続化)
  * **CLI統合**: `simonw/llm` のプラグイン機構
  * **フロントエンド**: HTML, CSS, JavaScript (軽量なフレームワーク/ライブラリ、例: Vanilla JS, Alpine.js, HTMXなど、またはシンプルなReact/Vue)
  * **非同期処理**: Python `asyncio`

#### アプリケーション構造

```
llm-agentchat/
├── llm_agentchat/
│   ├── __init__.py           # プラグインのエントリポイント、CLIコマンド定義
│   ├── server/
│   │   ├── __init__.py
│   │   ├── app.py            # FastAPI/Flaskアプリケーション（HTTP/WebSocketエンドポイント）
│   │   ├── db.py             # SQLiteデータベース操作モジュール
│   │   └── static/           # Web UIの静的ファイル (HTML, CSS, JS)
│   │       ├── index.html
│   │       └── script.js
│   ├── client/
│   │   ├── __init__.py
│   │   ├── agent.py          # LLMエージェントのコアロジック
│   │   └── websocket_client.py # WebSocketクライアントの実装
│   ├── config.py             # プラグインのデフォルト設定
├── setup.py                  # プラグインのインストール設定
├── agents.yml                # エージェント定義ファイル
├── README.md
└── requirements.txt
```

### コンポーネントとインターフェース

#### 主要コンポーネント

1.  **`agentchat-server` (バックエンド)**

      * **役割**: チャットルームのハブとして機能し、メッセージのルーティング、永続化、Web UIの提供を行います。
      * **機能**:
          * HTTP APIエンドポイントの提供 (`/api/messages`, `/api/message`, `/api/agents`)
          * WebSocketエンドポイントの提供 (`/ws`)
          * SQLiteデータベースとの連携（メッセージの読み書き）
          * Web UIの静的ファイル配信
      * **インターフェース**:
          * HTTPリクエスト/レスポンス (JSON形式)
          * WebSocketメッセージ (JSON形式)

2.  **`agentchat-client` (LLMエージェント)**

      * **役割**: `agents.yml`で定義されたエージェントとしてチャットルームに参加し、LLMを用いた対話を行います。
      * **機能**:
          * 指定された`agentchat-server`へのWebSocket接続確立
          * WebSocketからのメッセージ受信と解析
          * 自身のペルソナと会話履歴に基づいたLLM呼び出し（`simonw/llm`の内部APIを利用）
          * LLMからの応答をWebSocket経由でサーバーへ送信
          * ツール（`llm-code`など）の呼び出しロジック
      * **インターフェース**:
          * WebSocketクライアント
          * `simonw/llm` の内部LLM/ツール呼び出しAPI

3.  **Web UI (フロントエンド)**

      * **役割**: 人間がエージェント間の会話をリアルタイムで監視し、チャットに介入するためのインターフェースを提供します。
      * **機能**:
          * 過去のチャット履歴の表示
          * リアルタイムでの新しいメッセージ表示
          * メッセージ入力フォームと送信機能
          * チャットルーム参加エージェントのリスト表示
      * **インターフェース**:
          * JavaScriptの`WebSocket` API
          * JavaScriptの`fetch` API (HTTPリクエスト用)
          * HTML/CSSによるレイアウトとスタイリング

4.  **SQLiteデータベース**

      * **役割**: チャットルームのメッセージ履歴およびエージェントのセッション状態を永続化します。
      * **テーブル例**:
          * `messages`: `id`, `room_name`, `sender`, `message_content`, `timestamp`, `message_type`
          * `agents_state` (オプション): `room_name`, `agent_name`, `last_seen_timestamp`, `status`

#### インターフェース定義

```typescript
// メッセージオブジェクトの型定義 (WebSocketおよびHTTP APIで使用)
interface ChatMessage {
  room: string;           // チャットルームの名前
  sender: string;         // 送信者 (エージェント名または "human")
  message: string;        // メッセージ内容
  timestamp: string;      // ISO 8601形式のタイムスタンプ
  type: "chat" | "command" | "system"; // メッセージの種類
}

// エージェント定義ファイル (agents.yml) の型定義
interface AgentConfig {
  name: string;           // エージェント名
  model: string;          // 使用するLLMモデル名 (llm CLIで設定済みのもの)
  persona: string;        // エージェントのペルソナ/システムプロンプト
  tools?: string[];       // エージェントが利用できるツール名 (llm CLIプラグインとして提供)
  temperature?: number;   // LLMの温度設定
  max_tokens?: number;    // LLMの最大出力トークン数
  options?: { [key: string]: any }; // モデル固有のオプション (例: { "google_search": 1 })
}

interface AgentsConfigFile {
  agents: AgentConfig[];
  common_settings?: {
    chat_history_limit?: number; // LLMに渡す会話履歴のターン数
    response_delay_ms?: number;  // エージェントの応答前の最小遅延（人間が見やすいように）
  };
}
```

### データモデル

#### チャットメッセージデータ

  * **`messages` テーブル**:
      * `id` (INTEGER PRIMARY KEY AUTOINCREMENT)
      * `room_name` (TEXT NOT NULL): 所属するチャットルーム名
      * `sender` (TEXT NOT NULL): メッセージ送信者 (例: "プログラマーAgent", "human")
      * `message_content` (TEXT NOT NULL): メッセージ本文
      * `timestamp` (TEXT NOT NULL): メッセージ送信日時 (ISO 8601形式)
      * `message_type` (TEXT NOT NULL): メッセージの種類 ("chat", "command", "system"など)

#### エージェント設定データ

  * `agents.yml` ファイルによって定義されます。
  * 各エージェントは、固有の名前、使用するLLMモデル、詳細なペルソナ（システムプロンプト）、利用可能なツール、およびモデル固有のオプション（例: `llm-gemini`の`google_search`）を持ちます。
  * 共通設定により、会話履歴の制限や応答遅延などの振る舞いを一元的に管理できます。

#### 状態管理

  * **サーバーサイド**:
      * チャットメッセージは**SQLiteデータベース**に永続化されます。
      * アクティブなWebSocket接続、ルームごとのクライアントリストなどのセッション情報は、サーバーの**インメモリ**で管理されます。
  * **クライアントサイド (エージェント)**:
      * 各エージェントは自身の会話履歴を**インメモリ**で管理し、LLMへのコンテキストとして利用します。履歴の長さは`chat_history_limit`で制御されます。
  * **Web UI**:
      * 表示されるチャットメッセージリストはJavaScriptの**配列としてインメモリ**で管理され、WebSocketからの更新に応じて追加されます。

### エラーハンドリング

#### サーバーエラー

  * **ポート競合**: サーバー起動時に指定ポートが既に使用されている場合、エラーメッセージを表示して終了します。
  * **データベース接続エラー**: SQLiteデータベースファイルへのアクセス失敗時、適切なエラーをログに記録し、アプリケーションの動作を停止または制限します。
  * **APIリクエストエラー**: 不正な形式のAPIリクエストに対しては、HTTPステータスコード400 (Bad Request) などの適切なエラーレスポンスを返します。
  * **内部サーバーエラー**: 予期せぬ例外が発生した場合、HTTPステータスコード500 (Internal Server Error) を返し、詳細なエラー情報をログに記録します。

#### クライアント（エージェント）エラー

  * **サーバー接続失敗**: WebSocketまたはHTTP APIへの接続が確立できない場合、再試行ロジックを実装するか、ユーザーにエラーを通知します。
  * **LLM APIエラー**: LLMからの応答がエラー（APIキー無効、レートリミットなど）の場合、エラーメッセージをチャットルームに投稿するか、ログに記録します。
  * **ツール実行エラー**: エージェントがツールを呼び出した際にエラーが発生した場合、その旨をチャットで報告するか、ログに記録します。
  * **エージェント設定エラー**: `agents.yml` ファイルの読み込み失敗や不正な設定値の場合、エージェントの起動を中止し、エラーメッセージを表示します。

#### Web UIエラー

  * **WebSocket接続切断**: サーバーとのWebSocket接続が切断された場合、ユーザーに通知し、再接続を試みるか、再読み込みを促します。
  * **API通信エラー**: 過去のメッセージ取得などのHTTP API呼び出しが失敗した場合、エラーメッセージを表示します。
  * **表示エラー**: 予期せぬデータ形式やJavaScriptエラーが発生した場合、コンソールにエラーをログし、UIがクラッシュしないようフォールバックを提供します。

### パフォーマンス最適化

#### リアルタイム通信

  * **WebSocketの効率的な利用**: メッセージの送受信にWebSocketを優先し、HTTPポーリングを避けることで、レイテンシを最小限に抑えます。
  * **メッセージのバッチ処理**: 大量のメッセージが短時間に発生した場合、クライアント側で表示更新をバッチ処理することで、UIのフリーズを防ぎます。

#### データベースアクセス

  * **インデックスの利用**: `room_name` や `timestamp` カラムに**インデックスを適用**し、メッセージの検索と取得を高速化します。
  * **クエリの最適化**: 過去メッセージの取得は、最新のN件に限定するなどの最適化を行います。

#### LLM呼び出し

  * **非同期処理**: 複数のエージェントが同時にLLMを呼び出す可能性があるため、`asyncio` を用いた非同期処理でI/Oブロッキングを回避し、並行性を高めます。
  * **レートリミット対応**: LLMプロバイダーのレートリミットに達した場合、**指数バックオフ**などのリトライ戦略を実装します。
  * **会話履歴の管理**: LLMに渡す会話履歴の長さを適切に制限し、トークンコストとレイテンシを最適化します。

### テスト戦略

#### 単体テスト

  * **サーバーロジック**: HTTPエンドポイントのルーティング、WebSocketメッセージの処理、データベース操作（モックを使用）のテスト。
  * **クライアントロジック**: エージェントのメッセージ解析、LLMへのプロンプト生成ロジック、ツール呼び出しロジックのテスト。
  * **データモデル**: メッセージ構造、エージェント設定のパースとバリデーションのテスト。
  * **ユーティリティ関数**: タイムスタンプ生成、メッセージ整形などのヘルパー関数のテスト。

#### 統合テスト

  * **サーバー-クライアント通信**: サーバーとクライアント（エージェント）間でメッセージが正しく送受信されるか、WebSocket接続が維持されるかのテスト。
  * **LLM連携**: エージェントがLLMを呼び出し、適切な応答を受け取れるかのテスト（モックLLMまたは実際のLLM APIを使用）。
  * **データベース連携**: メッセージが正しくデータベースに保存され、取得できるかのテスト。
  * **Web UIとの連携**: Web UIがサーバーからメッセージをリアルタイムで受信し、表示を更新できるかのテスト。

#### E2Eテスト考慮事項

  * **主要なユーザーフロー**:
      * `llm agentchat-server` コマンドの実行とWeb UIの自動起動。
      * `llm agentchat-client` コマンドの実行とエージェントのチャット参加。
      * 人間がWeb UIからメッセージを送信し、エージェントが応答する一連の流れ。
      * 複数のエージェントが相互に会話する様子がWeb UIに表示されること。
  * **ツール連携**: エージェントが`llm-code`などのツールを呼び出し、その結果がチャットに反映されること。
  * **分散環境**: 異なるPCでサーバーとクライアントを起動し、正常に通信できるかのテスト。

### 実装詳細

#### CLI統合

  * `setup.py`の`entry_points`を使用して、`llm agentchat-server`と`llm agentchat-client`コマンドを`llm` CLIに登録します。
  * `llm`の内部API（`llm.models.get()`, `llm.keys.get()`など）を適切に利用し、既存のLLMエコシステムとシームレスに連携させます。

#### 非同期プログラミング

  * サーバーサイドではFastAPI（またはASGI互換フレームワーク）と`async/await`構文を全面的に採用し、多数の同時接続を効率的に処理します。
  * クライアントサイドのエージェントも、WebSocket通信とLLM呼び出しを非同期で行い、複数のエージェントが並行して動作できるようにします。

#### Web UIのレスポンシブ対応

  * シンプルなHTML/CSSで基本的なレイアウトを構築し、FlexboxやGridを使用して様々な画面サイズに対応させます。
  * モバイルデバイスでのタッチ操作を考慮し、入力フォームやスクロール領域のUI/UXを最適化します。
  * JavaScriptでWebSocketからのメッセージを効率的にDOMに追加し、スムーズなスクロール体験を提供します。

### パフォーマンス考慮事項

#### レイテンシ

  * LLMの応答時間は外部APIに依存するため、`response_delay_ms`などの設定で人間が見やすいように調整します。
  * WebSocketの利用により、メッセージ伝達のレイテンシを最小限に抑えます。

#### スケーラビリティ

  * SQLiteは単一ファイルデータベースのため、非常に大規模なメッセージ量や同時接続には限界があります。必要に応じて、PostgreSQLなどの本格的なデータベースへの移行を検討します。
  * エージェントの数は、利用するLLMプロバイダーのレートリミットや、サーバーのリソース（CPU, メモリ）によって制約されます。

#### リソース管理

  * 各エージェントプロセスは独立してLLMを呼び出すため、LLMのAPIコストとローカルリソース消費（CPU, メモリ）を監視できるようなデバッグオプションを提供します。
  * チャット履歴のデータベースは定期的にクリーンアップするか、サイズ制限を設けることを検討します。

